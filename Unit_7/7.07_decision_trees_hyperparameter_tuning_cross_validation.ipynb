{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7613d3b-1f88-41da-95f6-7f9bbd3d572d",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "- Describe how Decision Trees solve regression and classification problems\n",
    "- Implement simple decision trees\n",
    "- Identify and use key parameters while developing a decision tree model\n",
    "- Understand cross validation technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ae485-3bd9-47bc-88cf-40bd5f299fa9",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "- Decision trees can be used both for regression and classification problems\n",
    "\n",
    "- They work with stratifying or segmenting the predictor space into a number of binary decisions to make the prediction. Each binary split consists of a decision rule which either sends us left or sends us right. This is the basic structure of a decision tree: [link to the image - Decision Tree](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.07/7.07-decision_tree_terminology.png)\n",
    "\n",
    "\n",
    "- These are not as competitive as algorithms including random forests, bagging and boosting, which comprise of building hundreds or thousands of trees and then aggregating the results to yield a single prediction. We will take a look at these methods later this week. But decision trees form the basis of those aforementioned algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1becd1-8bdb-4aa9-82bd-931b961adc3b",
   "metadata": {},
   "source": [
    "#### Decision Trees for Regression\n",
    "- Here we will take the example of Boston housing data as it is a simpler regression problem. We used the Boston data from sklearn datasets in 7.6 as well. The objective was to predict the median value of a house.\n",
    "\n",
    "- To simplify the case even further, we will take a look at the example where we have to predict the median price of a house based on only one feature lstat.\n",
    "\n",
    "- A decision rule to make the prediction is shown here: [link to the image - Regression Tree Decision Split](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.07/7.07-regression_trees.png)\n",
    "\n",
    "- After we train the model, this is the decision space we get. The set of bottom nodes in the decision tree gives us the partition of the feature space into disjoint regions.\n",
    "\n",
    "- For each region, we calculate the average of the target variable falling in that region of the training data. That gives us the numerical prediction value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a293b-dec4-49a4-b667-3d8147d245ee",
   "metadata": {},
   "source": [
    "#### Decision Trees for Classification\n",
    "- Classification trees works in the same way as the regression trees except that instead of the final prediction being the mean of the target values falling in the disjointed region at the end, here the final prediction is the most occurring class in that region. [link to the image - Classification Tree with multiple features](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.07/7.07-classification_tree.png)\n",
    "\n",
    "\n",
    "- In the example above, instead of having one predictor we have multiple predictors. The decision space is divided among them. The decision at the bottom node still follows the same methodology. The final prediction is the most occurring class in the bottom nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7488e-40ed-4ab9-bd7b-882a035f4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee3cf3-9837-4c1c-8fc1-dd40b03afb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe410cb-ca18-4f41-974d-a0da4eb6b8da",
   "metadata": {},
   "source": [
    "#### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040bb305-5dc4-4cfa-a5c8-8c5e8b8991c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aba9a3-b51c-4a0e-9bc1-062858cc73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e688fae-e9dc-4f0b-8403-32d5c02ac729",
   "metadata": {},
   "source": [
    "#### Intro to Parameters\n",
    "- In the last lesson, we implemented decision trees for classification and regression. When we initialized the model, we did not pass any arguments to the function. We chose to work with default parameters. However there is a bunch of parameters that sklearn provides. These parameters can be adjusted to better suit our requirement and to the data, to improve efficiency and accuracy of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3769ce-8349-4b58-ac51-4ba09085afb9",
   "metadata": {},
   "source": [
    "- `criterion{“gini”, “entropy”}, default=”gini”` - Defines the criteria for decision split, ie., `gini` index vs. entropy.\n",
    "- `min_samples_split: int or float, default=2` - This is the minimum number of training samples at a decision split point, if it is to be further split into children nodes.\n",
    "- `min_samples_leaf: int or float, default=1` - This is the minimum number of training samples at a decision split point, if it is to be further split into leaf nodes.\n",
    "  :exclamation: Note for instructor: `min_samples_split` and `min_samples_leaf`, they look very similar but difference is between children node and leaf node. Children node can be split further while a leaf node can't be.\n",
    "- `max_depthint, default=None` - Defines the maximum depth of the tree. Each level of the decision split can be thought about as a depth level where the root node signifies level 0, next internal node as level 1 and so on and so forth.\n",
    "- `max_featuresint, float or {“auto”, “sqrt”, “log2”}, default=None` - This defines the maximum number of features to pick up every time when comparing the `gini` index or the entropy criteria for choosing the feature to make the split decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3c042-e01f-41c0-995b-f20efaad5d97",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "- When we built regression tree model and classification tree model, we observed that every time we ran the algorithm it gave us a slightly different result.\n",
    "\n",
    "- To achieve a better estimate of the result/accuracy/performance of the model, we perform cross validation which basically repeats the process a number of times by randomly shuffling the dataset and fitting the model and checking the accuracy. [link to the image - Validation Set](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.07/7.07-validation_set.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5372788-d521-4278-8615-e44062833688",
   "metadata": {},
   "source": [
    "##### Leave One Out Cross Validation\n",
    "- This method involves splitting the training data into two parts:\n",
    "\n",
    "  - a single observation for eg. `(x1,y1)` that is used for the validation set, and\n",
    "  - the remaining observations `{(x2, y2), . . . , (xn, yn)}` that are used for training the model. Model accuracy is calculated for this data now.\n",
    "\n",
    "- After this, another row is picked as validation set and the rest of the information for training the model. Model accuracy is calculated\n",
    "- This process is repeated 'n' number of times.\n",
    "- Average of all the accuracy measures is taken to get the final estimate\n",
    "\n",
    "- One disadvantage with this method is that, since we are calculating the MSE for only observation at a time, the result is a poor estimate as it is by nature highly variable: [link to the image - LOOCV](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.07/7.07-LOOCV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409801e-744c-4f9f-b483-3f21eca32443",
   "metadata": {},
   "source": [
    "##### K Fold Cross Validation\n",
    "- This method involves splitting the data randomly into `k` groups/folds, of approximately equal size. The first fold is treated as a validation set, and the model is fit on the remaining `k − 1` folds, which is our remaining data. The mean squared error, MSE, is then computed on the observations in the held-out/validation fold.\n",
    "- After this, another fold is picked as validation set and the rest of the information for training the model. Model accuracy is calculated.\n",
    "- This process is repeated `k` number of times, ie., once for each fold.\n",
    "- Average of all the accuracy measures is taken to get the final estimate.\n",
    "- Typically, given these considerations, one performs k-fold cross-validation using `k = 5` or `k = 10`, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance. [link to the image - K Fold Cross Validation](https://education-team-2020.s3-eu-west-1.amazonaws.com/data-analytics/7.07/7.07-k_fold_CV.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c16d1e-b834-4ad8-9b09-bc33480fc179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
